{"cells":[{"cell_type":"markdown","metadata":{"id":"qdCkH2JM07eu"},"source":["# Quasi-Newton methods: BFGS"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"T9vAduWLjiTl"},"outputs":[],"source":["import numpy as np\n","import jax\n","import jax.numpy as jnp\n","import scipy.optimize\n","import scipy as sp\n","import matplotlib.pyplot as plt\n","\n","# We enable double precision in JAX\n","from jax.config import config\n","config.update(\"jax_enable_x64\", True)"]},{"cell_type":"markdown","metadata":{"id":"VQmGG5duO6ig"},"source":["Consider the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function), that is minimized in $\\mathbf{x} = (1,1,\\dots,1)^T$:\n","\n","$$\\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^{N-1} [100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2]$$"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ybmumhr1z32F"},"outputs":[],"source":["def loss(x):\n","    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"opnXVnczPDLj"},"source":["Use `jax` to compute and compile the Rosenbrock function and its gradient."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["grad_jit = jax.jit(jax.grad(loss))\n","loss_jit = jax.jit(loss)\n"]},{"cell_type":"markdown","metadata":{"id":"CEvFRykGQtzx"},"source":["Implement the BFGS method (with line search) for the minimization of the Rosenbrock function.\n","Set a maximum of 1000 epochs and a stopping tolerance on the gradient eucledian norm of $10^{-8}$. Employ an initial guess for $\\mathbf{x}$ with random numbers in the interval $[0,2]$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3IJUnuTZdS1"},"outputs":[],"source":["N = 100\n","max_epochs = 1000\n","tol = 1e-8\n","\n","np.random.seed(0)\n","x = np.random.rand(N)*2\n","\n","grad = grad_jit(x)\n","epoch = 0\n","I = np.identity(N)\n","Binv = I.copy()\n","history = [loss_jit(x)]\n","\n","while ...\n","    epoch += 1\n","    # search direction\n","    p = ...\n","    \n","    # line search\n","    line_search = sp.optimize.line_search(loss_jit, grad_jit, x, p)\n","    alpha = line_search[0]\n","    x_new = x + alpha * p\n","\n","    ..."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyObgaPNyGizPxRKSWVCGz56","collapsed_sections":[],"name":"BFGS.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}
